{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compact-notice",
   "metadata": {},
   "source": [
    "# Softmax regression for handwritten digits\n",
    "\n",
    "Today, we'll implement a softmax classifier recognizing handwritten digits.  We'll begin by using a relatively small collection (around 1800) of low resolution (8 by 8pix) digits.  This can be easily acquired using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-gnome",
   "metadata": {},
   "source": [
    "The digits appear as an $m\\times n$ array, where $m$ is the number of data instances and $n$ is the number of features.  It's important to recognize that for this problem, the number of features is $8\\times8 = 64$: the instances are flattened.  If you want to plot a digit from the dataset using, for example, matplotlib's imshow, you'll need to reshape this.  \n",
    "\n",
    "You'll also want to be careful to normalize the data, preferably by subtracting the mean and dividing by the standard deviation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Perform normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-anthony",
   "metadata": {},
   "source": [
    "The labels appear as integers.  Write and apply a function that converts from this integer representation to a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Convert the labels to a one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-wellington",
   "metadata": {},
   "source": [
    "Another important step is to split the dataset into training and testing sets.  I like using the function sklearn.model_selection.train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-climate",
   "metadata": {},
   "source": [
    "With data in hand, we now need to implement the model.  Recall that our predictions will be computed as\n",
    "$$\n",
    "Y_{pred} = \\mathrm{Softmax}(\\Phi W)\n",
    "$$\n",
    "Implement the softmax method, generate the matrix $\\Phi$ (I suggest a linear model, which is to say that all you need to do will be to prepend a column of ones to the $m\\times n$ matrix of pixel values, and instantiate the parameter matrix $W$ (I suggest instantiating to an array of very small random numbers).  Your implementation of Softmax should be vectorized, in that it should take a $m \\times N$ array of logits and output and $m \\times N$ array without using a loop.  Make a prediction using this untrained model: a sensible result at this stage is that all classes are approximately equally likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-capital",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "raising-effectiveness",
   "metadata": {},
   "source": [
    "Now generate functions (or one function with multiple outputs) to compute the categorical cross entropy and its gradient.  These are given by \n",
    "$$\n",
    "\\mathcal{L}(W,\\Phi,Y_{obs}) = -\\frac{1}{mN} \\sum_{i=1}^m \\left(Y_{obs,i} \\ln \\mathrm{Softmax}(\\Phi W)\\right).\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\frac{ \\partial \\mathcal{L}}{\\partial W} = -\\frac{1}{mN} \\sum_{i=1}^m \\left[(Y_{obs,i} - \\mathrm{Softmax}(\\Phi W)_i) \\Phi_i^T\\right]^T. \n",
    "$$\n",
    "As you implement these functions, consider how to do so in as efficient a manner as possible.  Note that it is possible to vectorize the sums.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-czech",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "selective-minority",
   "metadata": {},
   "source": [
    "Implement gradient descent and train this model.  Record the value of $\\mathcal{L}$ as a function of gradient descent iteration, and produce a plot convincing yourself that the model is converging to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-filing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "static-albert",
   "metadata": {},
   "source": [
    "One very interesting result of working with image data is that we can interpret the learned parameters as images (the weight matrix is $N\\times (1+n)$.  If you get rid of the first entry, which corresponds to a constant offset, the remaining $N \\times n$ weights are each associated with a given input pixel for a given class).  Plot your weights as images (there should be ten of them).  Evaluate the pattern that you find.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-adventure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-allergy",
   "metadata": {},
   "source": [
    "Finally, once this task is complete, scale your method up to the larger (in both number of instances and resolution) dataset MNIST (you can get it using the command sklearn.datasets.fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)).  This will take substantial time to train!  Only do this once you are satisfied with your implementation on the digits dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-jones",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
